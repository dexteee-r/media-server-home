# ğŸ—“ï¸ Journal de bord 

note de rappel pur plus tard : 
installer une VM tiny win11

## **Date: 19/10/25 : lancement du projet :**

j'ai rÃ©aliser 2 prompt context, un pour GPT (pour la partie recherche et documentation) et l'autre pour CLAUDE (pour la partie code et dev) 

- comparatif Jellyfin / Plex / Emby 
	choix final : 
- Comparatif Immich / PhotoPrism / Nextcloud Photos â†’ gestion photos.
	choix final : 

- Tableau TrueNAS / OpenMediaVault / MinIO.
	choix final : 



## **Date: 21/10/2025**
DÃ©cisions:
  - Architecture cible: Proxmox VE + VM â€œServicesâ€ Docker (Option A), migration possible vers LXC plus tard.

  - Stack initiale: Traefik, Jellyfin, Immich (+ Postgres), Prometheus/Grafana, Watchtower, Tailscale/WireGuard, Restic/Borg.

  - Stockage recommandÃ©: ZFS (datasets media, photos, appdata, backups).
  Arguments clÃ©s:

  - Besoin VMs + Docker â†’ Proxmox simplifie lâ€™orchestration, snapshots et GPU passthrough.

  - SÃ©curitÃ© & simplicitÃ©: accÃ¨s distant via VPN au dÃ©part, pas dâ€™exposition publique.
  - Ã€ faire (prochaine session):

  - RÃ©diger ADR-001 (hyperviseur), tableau â€œservices/ports/volumesâ€, dÃ©finir stratÃ©gie de sauvegarde.
  - Questions/donnÃ©es attendues:

  - SpÃ©cs machine + tests rapides (CPU/GPU/RAM/disques/rÃ©seau) pour ajuster transcodage et ZFS.


## **Date 22/10/2025**
DÃ©cisions:
  - ajout d'un hdd de 500go dans la machine
  - changment de ram initialement 8go mtn -> 16go 
  - ces composants proviennent d'autre machines plus utiliser, j'ai donc fait du recyclage.
  - crÃ©tation de l'arbo du projet a upload dans GitHub
  -  utilisation de **Docker Compose** pour lâ€™orchestration des services.  
  - Raisons : standard DevOps, simplicitÃ© de maintenance, portabilitÃ©, compatibilitÃ© Traefik.  
  - Ã‰tape suivante : comparaison des reverse-proxy (ADR-003) et rÃ©daction du `docker-compose.yml` minimal.




## **Date : 26 et 27/10/2025**
DÃ©cisions prises :

- **Architecture globale** : Media-server domestique sur Proxmox VE avec VM Ubuntu + Docker Compose (Jellyfin + Immich + Traefik)
  
- **Hyperviseur** : Proxmox VE choisi pour virtualisation, gestion GPU passthrough et Ã©volutivitÃ© future

- **Stockage** : Installation Proxmox sur SSD NVMe 256GB (Samsung MZVLW256), effacement complet de Windows 11, HDD rÃ©servÃ© pour bibliothÃ¨ques mÃ©dias

- **Partitionnement** : 
  - Filesystem ext4
  - 30GB root Proxmox
  - 4GB swap
  - ~200GB pour VMs/containers
  - 16GB rÃ©serve systÃ¨me

- **RÃ©seau** : Configuration IP statique 192.168.1.100/24, gateway 192.168.1.1, connexion Ethernet temporaire (WiFi Ã  configurer post-installation)

- **BIOS** : Passage du mode SATA de RAID/RST vers AHCI pour dÃ©tection du SSD NVMe par l'installateur Proxmox

- **GPU Passthrough** : 
  - Activation IOMMU Intel (intel_iommu=on iommu=pt dans GRUB)
  - Chargement modules VFIO (vfio, vfio_iommu_type1, vfio_pci, vfio_virqfd)
  - Blacklist driver i915 pour libÃ©rer Intel HD Graphics 530
  - ID GPU forcÃ© vers VFIO-PCI (8086:1912)
  - Objectif : Transcodage hardware H.264/H.265 via QuickSync dans Jellyfin

- **DÃ©pÃ´ts APT** : DÃ©sactivation repo enterprise, activation pve-no-subscription pour mises Ã  jour gratuites

- **MatÃ©riel cible** :
  - Dell OptiPlex 7040
  - CPU Intel Core i5-6500 (4 cores @ 3.2GHz, Skylake)
  - GPU Intel HD Graphics 530 (QuickSync support)
  - RAM 8GB DDR4-2133
  - SantÃ© systÃ¨me validÃ©e (aucune erreur matÃ©rielle)

ProblÃ¨mes rencontrÃ©s et rÃ©solus :

- SSD non dÃ©tectÃ© initialement â†’ RÃ©solu via changement BIOS SATA en mode AHCI
- Driver i915 persistant malgrÃ© blacklist â†’ Tentative de bind forcÃ© VFIO-PCI en cours
- Erreur initramfs ESP sync â†’ Non bloquant, boot fonctionnel malgrÃ© warning

Prochaines Ã©tapes :

- Validation GPU passthrough (vÃ©rification `vfio-pci` actif)
- CrÃ©ation VM Ubuntu 24.04 LTS avec GPU assignÃ©
- DÃ©ploiement stack Docker (Jellyfin + Immich + PostgreSQL + Redis + Traefik)
- Configuration transcodage hardware QuickSync
- Scripts backup/restore automatisÃ©s
- Configuration WiFi permanente (post-Ethernet)


## Journal de bord â€“ 02/11/2025

### ğŸ”§ Mises Ã  jour systÃ¨me & infrastructure
- Passage des VMs sous **Debian 13**.
- Installation de **Docker** et **UFW** sur les deux VMs.
- Configuration du **pare-feu UFW** sur chaque VM avec politiques par dÃ©faut (`deny incoming`, `allow outgoing`).
- **VPN** configurÃ© : **OpenVPN** opÃ©rationnel sur la VM-EXTRANET.
- **Reverse Proxy** remplacÃ© : **Nginx Proxy Manager** (remplace Traefik).

### ğŸŒ RÃ©seau & DNS
- Achat du domaine **elmzn.be** chez **OVH Cloud**.
- Mise en place dâ€™un **DNS dynamique** via **OVH + ddclient** sur Debian.
- Ajout du sous-domaine : `intranet.elmzn.be` pour le rÃ©seau interne (INTRANET).
- Tests dâ€™accÃ¨s HTTPS internes validÃ©s via NPM.

### ğŸ§± Architecture & sÃ©curitÃ©
- Passage officiel en **multi-VM** :
  - **VM-EXTRANET** : Nginx Proxy Manager, OpenVPN, node_exporter, UFW + Fail2ban.
  - **VM-INTRANET** : Jellyfin, Immich, Postgres, Prometheus, Grafana, Restic.
- Objectifs :
  - RÃ©duire la surface dâ€™attaque.
  - Isoler les services internes et les donnÃ©es critiques.
  - Simplifier la restauration et les backups.
- Pare-feu Proxmox activÃ© sur **Datacenter + VM + Node**.
- Segmentation documentÃ©e entre les rÃ©seaux **vmbr0 (LAN)** et **vmbr1 (DMZ)**.

### ğŸ“˜ Documentation mise Ã  jour
- **ARCHITECTURE.md** : schÃ©ma multi-VM ajoutÃ©, flux inter-VM prÃ©cisÃ©s.
- **SECURITY.md** : Tailscale remplacÃ© par OpenVPN, ajout de la DMZ et flux inter-VM.
- **OPERATIONS.md** : procÃ©dures sÃ©parÃ©es (INTRANET / EXTRANET), ordres de restauration.
- **infra/proxmox/** : ajout des bridges et firewall.
- **infra/vm/** : crÃ©ation de `services-extranet.md` et `services-intranet.md`.
- **ADR-005** et **ADR-006** : ajout des sections *multi-VM adaptation* et *multi-VM monitoring*.
- **ADR-007** et **ADR-008** : ajoutÃ©s pour documenter la segmentation rÃ©seau et le placement des services.

### ğŸ—„ï¸ Sauvegardes & supervision
- **Restic** configurÃ© sur INTRANET (quotidien) et EXTRANET (hebdo).
- Snapshots ZFS automatiques activÃ©s sur le pool `tank`.
- Exporters installÃ©s : `node_exporter`, `smartctl_exporter`, `cadvisor`.
- Monitoring multi-VM opÃ©rationnel : Prometheus (INTRANET) scrape EXTRANET via port 9100.
- Dashboards Grafana mis Ã  jour et versionnÃ©s dans `/configs/grafana/dashboards/`.

### ğŸ§  SynthÃ¨se
- **VM-INTRANET** documentÃ©e : hÃ©berge Jellyfin, Immich, Postgres, Prometheus, Grafana, Restic.  
  Flux entrants limitÃ©s Ã  EXTRANET. Sauvegardes quotidiennes.  
- **VM-EXTRANET** documentÃ©e : NPM, OpenVPN, node_exporter, UFW + Fail2ban.  
  Flux sortants restreints, aucune donnÃ©e critique stockÃ©e localement.  
  Sauvegardes hebdomadaires exportÃ©es vers INTRANET.

### ğŸ“Œ Prochaines actions
- Mettre Ã  jour les configurations DNS publiques et privÃ©es (A / CNAME).  
- VÃ©rifier la restauration Restic par VM.  
- CrÃ©er le premier jeu de dashboards Grafana â€œInfrastructure Overviewâ€.

---
###ğŸ¯ Objectif initial
Mettre en place **tous les services de l'INTRANET** pour le projet **Media Server Home**.


## âœ… Ce qu'on a accompli

### 1. **ComprÃ©hension de l'architecture ZFS** ğŸ§ 
- **Question clÃ© :** Pourquoi crÃ©er les datasets ZFS sur l'hÃ´te Proxmox ?
- **RÃ©ponse :** 
  - ZFS a besoin d'accÃ¨s direct aux disques physiques
  - Snapshots centralisÃ©s
  - Partage entre VMs
  - Meilleures performances (ARC cache partagÃ©)
  - IntÃ©gritÃ© maximale (checksums, SMART)

---

### 2. **CrÃ©ation des pools ZFS** ğŸ’¾

#### Pool HDD (tank-hdd) - 450 GB
```bash
âœ… CrÃ©Ã© sur /dev/sda (HDD 500 GB complet)
âœ… Compression LZ4 activÃ©e
âœ… Atime dÃ©sactivÃ©
âœ… 4 datasets crÃ©Ã©s :
   - media (300 GB quota, recordsize=1M)
   - photos (150 GB quota)
   - backups (80 GB quota)
   - logs (20 GB quota)
```

#### Pool SSD (tank-ssd) - 15 GB
```bash
âœ… CrÃ©Ã© sur volume LVM /dev/pve/zfs-ssd
âœ… Option safe choisie (15 GB au lieu de 120 GB)
âœ… Pas de manipulation risquÃ©e du LVM
âœ… 2 datasets crÃ©Ã©s :
   - appdata (10 GB quota)
   - postgres (5 GB quota)
```

**Pourquoi 15 GB suffit :**
- Appdata : configs Docker (~5 GB max)
- Postgres : base Immich (~2-3 GB pour dÃ©marrer)
- DonnÃ©es volumineuses (photos/vidÃ©os) sur HDD

---

### 3. **CrÃ©ation des VMs** ğŸ–¥ï¸

#### VM-EXTRANET (ID 101)
```yaml
IP: 192.168.1.111
RAM: 4 GB
CPU: 2 cores
Disque: 20 GB
OS: Debian 13 (Trixie)
RÃ´le: DMZ / Porte d'entrÃ©e Internet
```

#### VM-INTRANET (ID 100) - existait dÃ©jÃ 
```yaml
IP: 192.168.1.101
RAM: 12 GB
CPU: 3 cores
Disque: 32 GB
OS: Debian 13 (Trixie)
RÃ´le: Services privÃ©s (Jellyfin, Immich, etc.)
```

**DÃ©cision stratÃ©gique :** CrÃ©er VM-EXTRANET **AVANT** le pool SSD pour Ã©viter de manipuler LVM deux fois.

---

### 4. **Tentative de bind mounts** âš ï¸

**ProblÃ¨me dÃ©couvert :** Les bind mounts Proxmox (`mp0:`) ne fonctionnent que pour les **conteneurs LXC**, pas pour les **VMs QEMU/KVM**.

```bash
âŒ Tentative : Ã‰diter /etc/pve/qemu-server/100.conf
âŒ RÃ©sultat : Montages n'apparaissent pas dans la VM
âœ… Solution : Passer Ã  NFS
```

---

### 5. **Configuration NFS** ğŸŒ

#### Serveur NFS (Proxmox)
```bash
âœ… Installation : nfs-kernel-server
âœ… Configuration : /etc/exports
âœ… 6 exports crÃ©Ã©s :
   - 5 pour VM-INTRANET (appdata, postgres, media, photos, backups)
   - 1 pour VM-EXTRANET (logs)
âœ… Service actif et vÃ©rifiÃ©
```

#### Client NFS (VM-INTRANET)
```bash
âœ… Installation : nfs-common
âœ… 5 montages NFS configurÃ©s
âœ… Ajout au /etc/fstab pour persistance
âœ… VÃ©rifiÃ© aprÃ¨s reboot : tous les montages OK
```

#### Client NFS (VM-EXTRANET)
```bash
âœ… Installation : nfs-common
âœ… 1 montage NFS configurÃ© (/mnt/logs)
âœ… Ajout au /etc/fstab
âœ… VÃ©rifiÃ© aprÃ¨s reboot : montage OK
```

---

## ğŸ“Š Architecture finale validÃ©e

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ PROXMOX VE 8.4 (192.168.1.100)                               â•‘
â•‘                                                               â•‘
â•‘ STOCKAGE ZFS                                                  â•‘
â•‘ â”œâ”€ tank-ssd (15 GB) - SSD NVMe                              â•‘
â•‘ â”‚  â”œâ”€ appdata  (10 GB)  â†’ NFS â†’ VM-INTRANET                 â•‘
â•‘ â”‚  â””â”€ postgres (5 GB)   â†’ NFS â†’ VM-INTRANET                 â•‘
â•‘ â””â”€ tank-hdd (450 GB) - HDD                                  â•‘
â•‘    â”œâ”€ media    (300 GB) â†’ NFS â†’ VM-INTRANET                 â•‘
â•‘    â”œâ”€ photos   (150 GB) â†’ NFS â†’ VM-INTRANET                 â•‘
â•‘    â”œâ”€ backups  (80 GB)  â†’ NFS â†’ VM-INTRANET                 â•‘
â•‘    â””â”€ logs     (20 GB)  â†’ NFS â†’ VM-EXTRANET                 â•‘
â•‘                                                               â•‘
â•‘ VMS                                                           â•‘
â•‘ â”œâ”€ VM-EXTRANET (192.168.1.111) - 4 GB RAM, 2 vCPU           â•‘
â•‘ â”‚  â””â”€ /mnt/logs (NFS) âœ…                                      â•‘
â•‘ â””â”€ VM-INTRANET (192.168.1.101) - 12 GB RAM, 3 vCPU          â•‘
â•‘    â”œâ”€ /mnt/appdata  (NFS) âœ…                                  â•‘
â•‘    â”œâ”€ /mnt/postgres (NFS) âœ…                                  â•‘
â•‘    â”œâ”€ /mnt/media    (NFS) âœ…                                  â•‘
â•‘    â”œâ”€ /mnt/photos   (NFS) âœ…                                  â•‘
â•‘    â””â”€ /mnt/backups  (NFS) âœ…                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ Ã‰tat actuel

```
âœ… Infrastructure Proxmox opÃ©rationnelle
âœ… Pools ZFS crÃ©Ã©s et optimisÃ©s
âœ… 2 VMs crÃ©Ã©es et configurÃ©es
âœ… NFS configurÃ© et persistant
âœ… Tous les montages testÃ©s et validÃ©s aprÃ¨s reboot
âœ… Docker dÃ©jÃ  installÃ© dans VM-INTRANET
```

---

## ğŸš€ Prochaines Ã©tapes : DÃ‰PLOIEMENT DES SERVICES

### VM-INTRANET
```yaml
Services Ã  dÃ©ployer :
- Jellyfin (streaming vidÃ©o/musique)
- Immich (gestion photos + app mobile)
- PostgreSQL (DB Immich)
- Redis (cache Immich)
- Prometheus (monitoring)
- Grafana (dashboards)
- node_exporter (mÃ©triques systÃ¨me)
```

### VM-EXTRANET
```yaml
Services Ã  dÃ©ployer :
- Nginx Proxy Manager (reverse proxy HTTPS)
- OpenVPN (VPN accÃ¨s distant)
- ddclient (DNS dynamique OVH)
- Fail2ban (protection bruteforce)
- UFW (firewall)
- node_exporter (mÃ©triques systÃ¨me)
```

---

## ğŸ’¡ DÃ©cisions techniques clÃ©s prises

| DÃ©cision | Choix | Raison |
|----------|-------|--------|
| **Ordre de crÃ©ation** | VMs â†’ Pools ZFS | Ã‰viter double manipulation LVM |
| **Taille pool SSD** | 15 GB (safe) | Pas de rÃ©duction LVM risquÃ©e |
| **Montage datasets** | NFS (pas bind mount) | Bind mounts = LXC only |
| **Persistance** | /etc/fstab | Montages auto aprÃ¨s reboot |
| **SÃ©curitÃ© NFS** | no_root_squash | AccÃ¨s complet depuis VMs |

---

## ğŸ“ˆ Temps estimÃ© restant

```
âœ… Infrastructure : 100% DONE
ğŸ”§ DÃ©ploiement services : ~1-2h
ğŸ”’ SÃ©curisation : ~30min
ğŸ§ª Tests & validation : ~30min
ğŸ“ Documentation finale : ~30min
```

---

## ğŸ‰ RÃ©sumÃ© en une phrase

**On a construit une infrastructure Proxmox + ZFS + NFS solide avec 2 VMs (EXTRANET + INTRANET), prÃªte Ã  accueillir tous les services Docker du Media Server !** ğŸš€


